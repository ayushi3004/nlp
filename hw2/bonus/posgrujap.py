# -*- coding: utf-8 -*-
"""POSgruJap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QerXGOwrvn_P64UDR252CtU9vgffX5OT
"""

import sys
import re
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding
from keras.layers import Dense, Input
from keras.layers import TimeDistributed
from keras.layers import GRU
from keras.models import Model
import tensorflow as tf
from tensorflow import keras
from keras import layers
from gensim.models import KeyedVectors

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)

MAX_LEN = 100
EMBEDDING_SIZE = 100

def get_data(tag_file, token_file):
    X = []
    Y = []
    with open(token_file) as tokenFile:
        for tokenString in tokenFile:
            X.append(tokenString)

    with open(tag_file) as tagFile:
        for tagString in tagFile:
            Y.append(tagString)
    return X, Y

def preprocess_data(X_train, Y_train, X_test, Y_test, unique_y_tags, word2vec):
    # vectorize X and Y 
    word_tokenizer = Tokenizer()

    word_tokenizer.fit_on_texts(X_train)
    wi = word_tokenizer.word_index
    VOC_LEN = len(wi) + 1
    X_encoded = word_tokenizer.texts_to_sequences(X_train)
    X_test_encoded = word_tokenizer.texts_to_sequences(X_test)
    
    tag_tokenizer = Tokenizer()
    tag_tokenizer.fit_on_texts(Y_train)
    Y_encoded = tag_tokenizer.texts_to_sequences(Y_train)
    Y_test_encoded = tag_tokenizer.texts_to_sequences(Y_test)

    # update embedding weights from word2vec model
    weights = np.zeros((VOC_LEN, EMBEDDING_SIZE))

    for word, index in wi.items():
        try:
            weights[index, :] = word2vec[word]
        except KeyError:
            pass

    # padding
    X_padded = sequence.pad_sequences(X_encoded, maxlen= MAX_LEN, padding="pre", truncating="post")
    X_test_padded = sequence.pad_sequences(X_test_encoded, maxlen= MAX_LEN, padding="pre", truncating="post")

    Y_padded = sequence.pad_sequences(Y_encoded, maxlen= MAX_LEN, padding="pre", truncating="post")
    Y_test_padded = sequence.pad_sequences(Y_test_encoded, maxlen= MAX_LEN, padding="pre", truncating="post")

    onehot_encoded_Y = tf.one_hot(Y_padded, unique_y_tags)
    onehot_encoded_test_Y = tf.one_hot(Y_test_padded, unique_y_tags)

    return np.array(X_padded), np.array(onehot_encoded_Y), np.array(X_test_padded), np.array(onehot_encoded_test_Y), weights, VOC_LEN

def train_model(X_train, Y_train, weights, VOC_LEN):
  NUM_CLASSES = Y_train.shape[2]

  gru_model = Sequential()

  emb = Embedding(input_dim =  VOC_LEN,         
                          output_dim    =  EMBEDDING_SIZE,
                          input_length  =  MAX_LEN,
                          trainable     =  False,
                          weights       = [weights],
                          mask_zero=True
  )

  gru_model.add(emb)

  gru_model.add(GRU(NUM_CLASSES,return_sequences=True))
  gru_model.add(GRU(NUM_CLASSES,return_sequences=True))

  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
  gru_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))
  opt = keras.optimizers.Adam(learning_rate=0.01)
  l = 'categorical_crossentropy'

  gru_model.compile(loss  =  l,
              optimizer =  opt,
              metrics   =  ['acc'])
  gru_model.fit(X_train, Y_train, callbacks=[callback], batch_size=100, epochs=10)
  return gru_model

def test(model, X_test, Y_test):
  Y_predict = model.predict(X_test)
  loss, accuracy = model.evaluate(X_test, Y_test)
  print(f"Loss: {loss},\nAccuracy: {accuracy}")
  return Y_predict, accuracy

TRAIN_TAG_FILE = "/content/drive/My Drive/nlp_files/hw2/jv.train.tgs"
TRAIN_TOKEN_FILE = "/content/drive/My Drive/nlp_files/hw2/jv.train.txt"
TEST_TAG_FILE = "/content/drive/My Drive/nlp_files/hw2/jv.test.tgs"
TEST_TOKEN_FILE = "/content/drive/My Drive/nlp_files/hw2/jv.test.txt"

X_1, Y_1 = get_data(TRAIN_TAG_FILE, TRAIN_TOKEN_FILE)
X_2, Y_2 = get_data(TEST_TAG_FILE, TEST_TOKEN_FILE)
unique_y_tags = len(set([word.lower() for sent in Y_1 for word in sent]))

# Load word2vec embeddings (glove twitter dataset)
path = "/content/drive/My Drive/nlp_files/hw2/gensim-data/glove-twitter-100/glove-twitter-100"
word2vec = KeyedVectors.load_word2vec_format(path, binary=False)

X_prep, Y_prep, X_test, Y_test, weights, VOC_LEN = preprocess_data(X_1, Y_1, X_2, Y_2, unique_y_tags, word2vec)

X_train, Y_train = X_prep, Y_prep

model = train_model(X_train, Y_train, weights, VOC_LEN)

Y_predict, accuracy = test(model, X_test, Y_test)

# Calculate word error
word_error = 1- accuracy
print("error rate by word: {}".format(word_error))

# Calculate sentence error
acc_sent = []
for i in range(len(Y_test)):
  sent = Y_test[i]
  sum = 0
  cnt = 0
  for j in range(len(sent)):
    tag = Y_test[i][j]
    if np.argmax(tag) == 0:
      continue
    k = np.argmax(tag) == np.argmax(Y_predict[i][j])
    sum += int(k)
    cnt += 1
  if cnt != 0:
    acc = sum/cnt
  acc_sent.append(acc)

sent_error = 1 - (np.sum(np.array(acc_sent) == 1.0)/len(acc_sent))
print("error rate by sentence: {}".format(sent_error))
